{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import joblib \n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor \n",
    "from xgboost import XGBClassifier, XGBRegressor \n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, \n",
    "    mean_squared_error, mean_absolute_error, r2_score \n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"--- Part 1: Setup & Configuration ---\")\n",
    "\n",
    "RAW_DATA_PATH = 'data/raw/afgh_may25.csv'\n",
    "TAKEOVER_DATE_STR = \"2021-08-15\"\n",
    "GRID_CELL_SIZE_DEG = 1.0\n",
    "HISTORY_LAGS_DAYS = [7, 30, 90]\n",
    "MIN_ACTOR_FREQ = 10 \n",
    "MAX_DAYS_SINCE_LAST_EVENT = 180 \n",
    "\n",
    "TEST_SET_FRAC = 0.15 \n",
    "VALIDATION_SET_FRAC_FROM_DEV = 0.15 \n",
    "\n",
    "N_CV_SPLITS = 3 \n",
    "PRIMARY_METRIC_CLASSIFICATION = 'f1_weighted' \n",
    "PRIMARY_METRIC_REGRESSION = 'neg_root_mean_squared_error' \n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed) \n",
    "\n",
    "RESULTS_DIR = 'results/'\n",
    "PLOTS_DIR = \"visualizations/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 2: Load and Initial Clean ACLED Data ---\n",
    "print(\"\\n--- Part 2: Load and Initial Clean ACLED Data ---\")\n",
    "try:\n",
    "    df = pd.read_csv(RAW_DATA_PATH, low_memory=False)\n",
    "    print(f\"Loaded ACLED data. Initial shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: ACLED data file '{RAW_DATA_PATH}' not found. Halting.\")\n",
    "    df = pd.DataFrame() \n",
    "\n",
    "if not df.empty:\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'], errors='coerce')\n",
    "    df.dropna(subset=['event_date'], inplace=True)\n",
    "\n",
    "    numeric_cols_convert = ['latitude', 'longitude', 'fatalities', 'geo_precision', 'time_precision']\n",
    "    for col in numeric_cols_convert:\n",
    "        if col in df.columns: \n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    df['fatalities'] = df['fatalities'].fillna(0).astype(int)\n",
    "    df['log_fatalities'] = np.log1p(df['fatalities'])\n",
    "\n",
    "    def classify_fatalities(x):\n",
    "        if x == 0: return 'none'\n",
    "        elif x == 1: return 'low'\n",
    "        else: return 'serious' \n",
    "    df['fatality_level'] = df['fatalities'].apply(classify_fatalities)\n",
    "    \n",
    "    df.sort_values('event_date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"Data initially cleaned. Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty after loading or initial cleaning. Skipping further steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 3: Feature Engineering ---\n",
    "print(\"\\n--- Part 3: Feature Engineering ---\")\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"Engineering temporal features...\")\n",
    "    df['year'] = df['event_date'].dt.year\n",
    "    df['month'] = df['event_date'].dt.month\n",
    "    df['dayofweek'] = df['event_date'].dt.dayofweek\n",
    "    df['dayofyear'] = df['event_date'].dt.dayofyear\n",
    "    df['is_post_takeover'] = (df['event_date'] >= pd.to_datetime(TAKEOVER_DATE_STR)).astype(int)\n",
    "\n",
    "    print(\"Engineering spatial features (grid_cell_id)...\")\n",
    "    def add_grid_cell_id(df_input, lat_col, lon_col, cell_size_deg):\n",
    "        df_out = df_input.copy()\n",
    "        df_with_coords = df_out.dropna(subset=[lat_col, lon_col]).copy()\n",
    "        \n",
    "        if df_with_coords.empty: \n",
    "            df_out['grid_cell_id'] = f\"cell_NaN_NaN\"\n",
    "            return df_out\n",
    "\n",
    "        min_lat, max_lat = df_with_coords[lat_col].min(), df_with_coords[lat_col].max()\n",
    "        min_lon, max_lon = df_with_coords[lon_col].min(), df_with_coords[lon_col].max()\n",
    "        \n",
    "        eps = 1e-9 \n",
    "        lat_bins = np.arange(min_lat, max_lat + cell_size_deg - eps, cell_size_deg)\n",
    "        lon_bins = np.arange(min_lon, max_lon + cell_size_deg - eps, cell_size_deg)\n",
    "\n",
    "        if len(lat_bins) < 2: lat_bins = np.array([min_lat, max_lat + eps])\n",
    "        if len(lon_bins) < 2: lon_bins = np.array([min_lon, max_lon + eps])\n",
    "\n",
    "        df_with_coords['lat_idx'] = pd.cut(df_with_coords[lat_col], bins=lat_bins, labels=False, include_lowest=True, right=False)\n",
    "        df_with_coords['lon_idx'] = pd.cut(df_with_coords[lon_col], bins=lon_bins, labels=False, include_lowest=True, right=False)\n",
    "        \n",
    "        df_with_coords['gc_temp'] = df_with_coords.apply(\n",
    "            lambda r: f\"c_{int(r['lat_idx'])}_{int(r['lon_idx'])}\" if pd.notna(r['lat_idx']) and pd.notna(r['lon_idx']) else \"c_NaN_NaN\", axis=1)\n",
    "        \n",
    "        df_out = df_out.join(df_with_coords[['gc_temp']])\n",
    "        df_out.rename(columns={'gc_temp': 'grid_cell_id'}, inplace=True)\n",
    "        df_out['grid_cell_id'].fillna(\"c_NaN_NaN\", inplace=True) \n",
    "        return df_out\n",
    "    df = add_grid_cell_id(df, 'latitude', 'longitude', GRID_CELL_SIZE_DEG)\n",
    "\n",
    "    print(\"Engineering actor features (actor1_grouped for rare actors)...\")\n",
    "    def encode_actors_grouped(df_in, actor_col, min_f):\n",
    "        df_o = df_in.copy()\n",
    "        nan_placeholder = \"ACTOR_UNKNOWN\"\n",
    "        df_o[actor_col] = df_o[actor_col].fillna(nan_placeholder)\n",
    "        \n",
    "        actor_counts = df_o[actor_col].value_counts()\n",
    "        rare_actors = actor_counts[actor_counts < min_f].index\n",
    "        \n",
    "        df_o[f'{actor_col}_grouped'] = df_o[actor_col].apply(\n",
    "            lambda x: 'Other_Actor' if x in rare_actors else x)\n",
    "        return df_o\n",
    "    df = encode_actors_grouped(df, 'actor1', MIN_ACTOR_FREQ)\n",
    "\n",
    "    print(\"Engineering lagged/trend features (event counts & fatality sums)...\")\n",
    "    def engineer_lagged_features(df_input, unit_col, date_col='event_date', lag_windows=None):\n",
    "        if lag_windows is None: lag_windows = [7, 30, 90]\n",
    "\n",
    "        df_out = df_input.copy()\n",
    "        id_col_for_count = 'event_id_cnty' if 'event_id_cnty' in df_out.columns else df_out.columns[0]\n",
    "        \n",
    "        daily_agg = df_out.groupby([unit_col, pd.Grouper(key=date_col, freq='D')], observed=False).agg(\n",
    "            _daily_event_count=(id_col_for_count, 'size'), \n",
    "            _daily_fatalities_sum=('fatalities', 'sum')\n",
    "        ).reset_index()\n",
    "        \n",
    "        daily_agg = daily_agg.set_index([unit_col, date_col]).sort_index()\n",
    "        \n",
    "        lagged_results_all_units = []\n",
    "        for unit_name, group_df_unit in daily_agg.groupby(level=0): \n",
    "            group_df_unit = group_df_unit.reset_index(level=0, drop=True) \n",
    "            for lag in lag_windows:\n",
    "                win = f'{lag}D' \n",
    "                group_df_unit[f'event_count_{unit_col}_lag{lag}d'] = group_df_unit['_daily_event_count'].shift(1).rolling(win, min_periods=1).sum().fillna(0).astype(int)\n",
    "                group_df_unit[f'sum_fatalities_{unit_col}_lag{lag}d'] = group_df_unit['_daily_fatalities_sum'].shift(1).rolling(win, min_periods=1).sum().fillna(0)\n",
    "            lagged_results_all_units.append(group_df_unit.reset_index().assign(**{unit_col: unit_name}))\n",
    "        \n",
    "        if not lagged_results_all_units: \n",
    "            return df_out\n",
    "\n",
    "        processed_daily_lags = pd.concat(lagged_results_all_units, ignore_index=True)\n",
    "        \n",
    "        df_out['_day_only_date'] = df_out[date_col].dt.normalize() \n",
    "        \n",
    "        cols_to_merge = [unit_col, date_col] + [col for col in processed_daily_lags.columns if 'lag' in col and unit_col in col]\n",
    "        cols_to_merge = list(dict.fromkeys(cols_to_merge))\n",
    "\n",
    "        if date_col in processed_daily_lags.columns: processed_daily_lags[date_col] = pd.to_datetime(processed_daily_lags[date_col]).dt.normalize()\n",
    "        df_out = pd.merge(df_out, processed_daily_lags[cols_to_merge], \n",
    "                          left_on=[unit_col, '_day_only_date'], right_on=[unit_col, date_col],\n",
    "                          how='left', suffixes=('', '_daily'))\n",
    "        if f'{date_col}_daily' in df_out.columns: df_out.drop(columns=[f'{date_col}_daily'], inplace=True)\n",
    "        df_out.drop(columns=['_day_only_date'], inplace=True)\n",
    "        \n",
    "        for col in df_out.columns:\n",
    "            if ('lag' in col) and (unit_col in col): df_out[col].fillna(0, inplace=True)\n",
    "        return df_out\n",
    "\n",
    "    if 'admin1' in df.columns: df = engineer_lagged_features(df, 'admin1', lag_windows=HISTORY_LAGS_DAYS)\n",
    "    if 'grid_cell_id' in df.columns: df = engineer_lagged_features(df, 'grid_cell_id', lag_windows=HISTORY_LAGS_DAYS)\n",
    "\n",
    "    print(\"Engineering time since last event features...\")\n",
    "    df.sort_values('event_date', inplace=True) \n",
    "    \n",
    "    if 'admin1' in df.columns:\n",
    "        df.sort_values(['admin1', 'event_date'], inplace=True)\n",
    "        df['days_since_last_event_in_admin1'] = df.groupby('admin1')['event_date'].diff().dt.days.fillna(MAX_DAYS_SINCE_LAST_EVENT).clip(upper=MAX_DAYS_SINCE_LAST_EVENT)\n",
    "    \n",
    "    if 'grid_cell_id' in df.columns:\n",
    "        df_temp_grid_ts = df[df['grid_cell_id'] != \"c_NaN_NaN\"].copy() \n",
    "        if not df_temp_grid_ts.empty:\n",
    "            df_temp_grid_ts.sort_values(['grid_cell_id', 'event_date'], inplace=True)\n",
    "            df_temp_grid_ts['tsle_grid'] = df_temp_grid_ts.groupby('grid_cell_id')['event_date'].diff().dt.days\n",
    "            df = df.set_index(df.index).join(df_temp_grid_ts[['tsle_grid']]) \n",
    "            df.rename(columns={'tsle_grid': 'days_since_last_event_in_grid_cell'}, inplace=True)\n",
    "        if 'days_since_last_event_in_grid_cell' not in df.columns: \n",
    "            df['days_since_last_event_in_grid_cell'] = MAX_DAYS_SINCE_LAST_EVENT\n",
    "        df['days_since_last_event_in_grid_cell'] = df['days_since_last_event_in_grid_cell'].fillna(MAX_DAYS_SINCE_LAST_EVENT).clip(upper=MAX_DAYS_SINCE_LAST_EVENT)\n",
    "\n",
    "    df.sort_values('event_date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Feature engineering complete. Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty. Skipping feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 4: Feature Selection & Defining X, y ---\n",
    "print(\"\\n--- Part 4: Feature Selection & Defining X, y ---\")\n",
    "\n",
    "if not df.empty:\n",
    "    TARGET_COL_CLASSIFICATION = 'fatality_level'\n",
    "    TARGET_COL_REGRESSION = 'log_fatalities' \n",
    "\n",
    "    categorical_features_for_ohe = [\n",
    "        'event_type', 'sub_event_type', 'admin1', 'grid_cell_id', 'actor1_grouped',\n",
    "        'year', 'month', 'dayofweek', 'disorder_type', 'interaction', 'source_scale'\n",
    "    ]\n",
    "    numerical_features_base = [\n",
    "        'latitude', 'longitude', 'geo_precision', 'time_precision', 'dayofyear', 'is_post_takeover'\n",
    "    ]\n",
    "    \n",
    "    lag_trend_time_since_features = [col for col in df.columns if ('lag' in col or 'trend' in col or 'days_since_last_event' in col)]\n",
    "\n",
    "    categorical_features = [col for col in categorical_features_for_ohe if col in df.columns]\n",
    "    numerical_features = list(dict.fromkeys([col for col in numerical_features_base if col in df.columns] + \n",
    "                                             [col for col in lag_trend_time_since_features if col in df.columns]))\n",
    "    \n",
    "    feature_columns = list(dict.fromkeys([col for col in (categorical_features + numerical_features) if col in df.columns]))\n",
    "\n",
    "    critical_originals_for_nan_check = ['latitude', 'longitude', 'geo_precision', 'time_precision', 'event_type', 'sub_event_type', 'admin1']\n",
    "    existing_critical_for_nan_check = [col for col in critical_originals_for_nan_check if col in df.columns]\n",
    "    if existing_critical_for_nan_check: \n",
    "        df.dropna(subset=existing_critical_for_nan_check, inplace=True)\n",
    "        print(f\"Dropped rows with NaNs in critical features. Shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"No critical original features found for NaN drop check.\")\n",
    "else:\n",
    "    print(\"DataFrame is empty. Skipping feature selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 5: Data Splitting ---\n",
    "print(\"\\n--- Part 5: Data Splitting ---\")\n",
    "\n",
    "if not df.empty:\n",
    "    df.sort_values('event_date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    n_total = len(df)\n",
    "    n_test = int(n_total * TEST_SET_FRAC)\n",
    "    n_dev = n_total - n_test \n",
    "\n",
    "    n_val = int(n_dev * VALIDATION_SET_FRAC_FROM_DEV) \n",
    "    n_train_cv = n_dev - n_val \n",
    "\n",
    "    df_train_cv = df.iloc[:n_train_cv].copy()\n",
    "    df_val_es = df.iloc[n_train_cv:n_dev].copy()\n",
    "    df_test = df.iloc[n_dev:].copy()\n",
    "\n",
    "    print(f\"Dataset split completed:\")\n",
    "    print(f\"  Train_CV set size: {len(df_train_cv)} events\")\n",
    "    print(f\"  Validation_ES set size: {len(df_val_es)} events\")\n",
    "    print(f\"  Test set size: {len(df_test)} events\")\n",
    "else:\n",
    "    print(\"DataFrame is empty. Skipping data splitting.\")\n",
    "    df_train_cv, df_val_es, df_test = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 6: Target Variable Encoding ---\n",
    "print(\"\\n--- Part 6: Target Variable Encoding ---\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "all_y_labels_list = []\n",
    "if not df_train_cv.empty: all_y_labels_list.extend(df_train_cv[TARGET_COL_CLASSIFICATION].tolist())\n",
    "if not df_val_es.empty: all_y_labels_list.extend(df_val_es[TARGET_COL_CLASSIFICATION].tolist())\n",
    "if not df_test.empty: all_y_labels_list.extend(df_test[TARGET_COL_CLASSIFICATION].tolist())\n",
    "\n",
    "if all_y_labels_list:\n",
    "    le.fit(list(set(all_y_labels_list)))\n",
    "else: \n",
    "    print(\"Warning: No labels found. Using default classes.\")\n",
    "    le.fit(['none','low','serious']) \n",
    "\n",
    "print(f\"LabelEncoder classes: {list(le.classes_)} -> {np.arange(len(le.classes_))}\")\n",
    "\n",
    "y_train_cv_class_encoded = pd.Series(le.transform(df_train_cv[TARGET_COL_CLASSIFICATION]), index=df_train_cv.index) if not df_train_cv.empty else pd.Series(dtype=int)\n",
    "y_val_es_class_encoded = pd.Series(le.transform(df_val_es[TARGET_COL_CLASSIFICATION]), index=df_val_es.index) if not df_val_es.empty else pd.Series(dtype=int)\n",
    "y_test_class_encoded = pd.Series(le.transform(df_test[TARGET_COL_CLASSIFICATION]), index=df_test.index) if not df_test.empty else pd.Series(dtype=int)\n",
    "\n",
    "y_train_cv_reg = df_train_cv[TARGET_COL_REGRESSION] if not df_train_cv.empty else pd.Series(dtype=float)\n",
    "y_val_es_reg = df_val_es[TARGET_COL_REGRESSION] if not df_val_es.empty else pd.Series(dtype=float)\n",
    "y_test_reg = df_test[TARGET_COL_REGRESSION] if not df_test.empty else pd.Series(dtype=float)\n",
    "\n",
    "print(\"Target variables encoded/assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 7: Feature Preprocessing ---\n",
    "print(\"\\n--- Part 7: Feature Preprocessing ---\")\n",
    "\n",
    "X_train_cv = df_train_cv[feature_columns] if not df_train_cv.empty else pd.DataFrame(columns=feature_columns)\n",
    "X_val_es = df_val_es[feature_columns] if not df_val_es.empty else pd.DataFrame(columns=feature_columns)\n",
    "X_test = df_test[feature_columns] if not df_test.empty else pd.DataFrame(columns=feature_columns)\n",
    "\n",
    "if not X_train_cv.empty:\n",
    "    categorical_features = [col for col in categorical_features if col in X_train_cv.columns and X_train_cv[col].nunique(dropna=False) > 0]\n",
    "    numerical_features = [col for col in numerical_features if col in X_train_cv.columns and (X_train_cv[col].nunique(dropna=False) > 1 if pd.api.types.is_numeric_dtype(X_train_cv[col]) else True) ]\n",
    "    \n",
    "    feature_columns = list(dict.fromkeys(categorical_features + numerical_features))\n",
    "    X_train_cv = X_train_cv[feature_columns]\n",
    "    X_val_es = X_val_es[feature_columns]\n",
    "    X_test = X_test[feature_columns]\n",
    "else: \n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    print(\"X_train_cv is empty, preprocessor will not be fitted meaningfully.\")\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=0.005), categorical_features),\n",
    "    ('num', StandardScaler(), numerical_features)], \n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "if not X_train_cv.empty:\n",
    "    print(\"Fitting preprocessor and transforming data...\")\n",
    "    X_train_cv_processed = preprocessor.fit_transform(X_train_cv)\n",
    "    \n",
    "    X_val_es_processed = preprocessor.transform(X_val_es) if not X_val_es.empty else np.array([]).reshape(0, X_train_cv_processed.shape[1])\n",
    "    X_test_processed = preprocessor.transform(X_test) if not X_test.empty else np.array([]).reshape(0, X_train_cv_processed.shape[1])\n",
    "    \n",
    "    processed_feature_names = preprocessor.get_feature_names_out()\n",
    "    \n",
    "    print(f\"Processed shapes: Train {X_train_cv_processed.shape}, Val {X_val_es_processed.shape}, Test {X_test_processed.shape}\")\n",
    "    print(f\"Number of features after OHE/Scaling: {X_train_cv_processed.shape[1]}\")\n",
    "else: \n",
    "    X_train_cv_processed = np.array([])\n",
    "    X_val_es_processed = np.array([])\n",
    "    X_test_processed = np.array([])\n",
    "    processed_feature_names = []\n",
    "    print(\"Skipped fitting/transforming due to empty X_train_cv.\")\n",
    "\n",
    "print(\"Feature preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 8: Model Training & Evaluation Pipeline Function ---\n",
    "print(\"\\n--- Part 8: Model Training & Evaluation Pipeline Function Definition ---\")\n",
    "\n",
    "def run_modeling_pipeline(\n",
    "    X_train_proc, y_train, \n",
    "    X_val_proc, y_val, \n",
    "    X_test_proc, y_test, \n",
    "    model_instance, model_name_prefix, task_type, \n",
    "    param_grid=None, cv_splitter=None, primary_scoring_metric=None, \n",
    "    feature_names_processed=None, random_state_seed=42\n",
    "):\n",
    "    \n",
    "    model_full_name = f\"{type(model_instance).__name__}{model_name_prefix}\"\n",
    "    print(f\"\\n--- Running Pipeline for: {model_full_name} ({task_type}) ---\")\n",
    "    results = {'Model': model_full_name, 'Task': task_type, 'CV_Score': np.nan, 'Best_Params': '{}'}\n",
    "    best_model = None\n",
    "    \n",
    "    current_model_instance = clone(model_instance)\n",
    "    if hasattr(current_model_instance, 'random_state') and current_model_instance.random_state is None:\n",
    "        current_model_instance.random_state = random_state_seed\n",
    "    if hasattr(current_model_instance, 'seed') and current_model_instance.seed is None:\n",
    "        current_model_instance.seed = random_state_seed\n",
    "\n",
    "    val_set_avail = isinstance(X_val_proc, np.ndarray) and X_val_proc.size > 0 and \\\n",
    "                    isinstance(y_val, (pd.Series, np.ndarray)) and len(y_val) > 0\n",
    "    \n",
    "    orig_model_uses_es = 'early_stopping_rounds' in model_instance.get_params() and \\\n",
    "                         model_instance.get_params()['early_stopping_rounds'] is not None\n",
    "\n",
    "    fit_params_gscv = {}\n",
    "    if orig_model_uses_es and val_set_avail:\n",
    "        fit_params_gscv['eval_set'] = [(X_val_proc, y_val)]\n",
    "        if type(current_model_instance).__name__.startswith('XGB'): \n",
    "            fit_params_gscv['verbose'] = False \n",
    "        print(\"    (Validation set for GridSearchCV early stopping)\")\n",
    "    elif orig_model_uses_es and not val_set_avail:\n",
    "        if hasattr(current_model_instance, 'early_stopping_rounds'): \n",
    "            current_model_instance.early_stopping_rounds = None\n",
    "        print(\"    (No validation set, ES disabled on clone)\")\n",
    "\n",
    "    if param_grid and cv_splitter and primary_scoring_metric:\n",
    "        print(f\"  Running GridSearchCV for {model_full_name}...\")\n",
    "        gscv = GridSearchCV(\n",
    "            current_model_instance, param_grid, cv=cv_splitter, \n",
    "            scoring=primary_scoring_metric, n_jobs=-1, verbose=0, error_score='raise'\n",
    "        )\n",
    "        try: \n",
    "            gscv.fit(X_train_proc, y_train, **fit_params_gscv)\n",
    "            best_model = gscv.best_estimator_\n",
    "            results['Best_Params'] = str(gscv.best_params_)\n",
    "            results['CV_Score'] = gscv.best_score_\n",
    "            print(f\"    Best parameters: {results['Best_Params']}\")\n",
    "            print(f\"    Best CV ({primary_scoring_metric}): {results['CV_Score']:.4f}\")\n",
    "        except Exception as e: \n",
    "            print(f\"    ERROR during GridSearchCV for {model_full_name}: {e}\\n    Fallback to direct fit.\")\n",
    "            best_model = None \n",
    "    \n",
    "    if best_model is None: \n",
    "        print(f\"  Training {model_full_name} (direct fit)...\")\n",
    "        fallback_model = clone(model_instance) \n",
    "        if hasattr(fallback_model, 'random_state') and fallback_model.random_state is None:\n",
    "            fallback_model.random_state = random_state_seed\n",
    "        if hasattr(fallback_model, 'seed') and fallback_model.seed is None:\n",
    "            fallback_model.seed = random_state_seed\n",
    "        try:\n",
    "            if orig_model_uses_es and val_set_avail: \n",
    "                fallback_model.fit(X_train_proc, y_train, eval_set=[(X_val_proc, y_val)], verbose=False)\n",
    "            elif orig_model_uses_es and not val_set_avail: \n",
    "                if hasattr(fallback_model, 'early_stopping_rounds'): fallback_model.early_stopping_rounds = None\n",
    "                fallback_model.fit(X_train_proc, y_train, verbose=False) \n",
    "            else: \n",
    "                fallback_model.fit(X_train_proc, y_train)\n",
    "            best_model = fallback_model\n",
    "        except Exception as e: \n",
    "            print(f\"    ERROR during direct fit for {model_full_name}: {e}\")\n",
    "            return {**results, 'ModelObject': None, 'Importances': None} \n",
    "    \n",
    "    if not best_model: \n",
    "        print(f\"MODEL TRAINING FAILED FOR {model_full_name}. Skipping evaluation.\")\n",
    "        return {**results, 'ModelObject': None, 'Importances': None}\n",
    "\n",
    "    results['ModelObject'] = best_model\n",
    "\n",
    "    if isinstance(X_test_proc, np.ndarray) and X_test_proc.size > 0 and \\\n",
    "       isinstance(y_test, (pd.Series, np.ndarray)) and len(y_test) > 0:\n",
    "        y_pred = best_model.predict(X_test_proc)\n",
    "        \n",
    "        if task_type == 'classification':\n",
    "            results.update({\n",
    "                'Accuracy': accuracy_score(y_test, y_pred), \n",
    "                'F1-Weighted': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                'Precision-W': precision_score(y_test, y_pred, average='weighted', zero_division=0), \n",
    "                'Recall-W': recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            })\n",
    "            if hasattr(best_model, \"predict_proba\"):\n",
    "                y_prob = best_model.predict_proba(X_test_proc)\n",
    "                roc_auc = np.nan\n",
    "                try:\n",
    "                    unique_labels_test = np.unique(y_test)\n",
    "                    if len(unique_labels_test) > 2 and y_prob.shape[1] == len(unique_labels_test): \n",
    "                        roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='weighted', labels=unique_labels_test)\n",
    "                    elif y_prob.shape[1] == 2: \n",
    "                        roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "                except ValueError as e_roc: \n",
    "                    print(f\"  ROC AUC error for {model_full_name}: {e_roc}\")\n",
    "                results['ROC_AUC'] = roc_auc\n",
    "            print(f\"    Test Metrics - F1-W: {results.get('F1-Weighted',np.nan):.4f}, Accuracy: {results.get('Accuracy',np.nan):.4f}, ROC_AUC: {results.get('ROC_AUC',np.nan):.4f}\")\n",
    "        \n",
    "        elif task_type == 'regression':\n",
    "            results.update({\n",
    "                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)), \n",
    "                'MAE': mean_absolute_error(y_test, y_pred), \n",
    "                'R2': r2_score(y_test, y_pred)\n",
    "            })\n",
    "            print(f\"    Test Metrics - RMSE: {results.get('RMSE',np.nan):.4f}, R2: {results.get('R2',np.nan):.4f}\")\n",
    "    else: \n",
    "        print(f\"  Test set empty or invalid for {model_full_name}, skipping test evaluation.\")\n",
    "\n",
    "    fi_df = None\n",
    "    importances_data = None\n",
    "    if hasattr(best_model, 'feature_importances_'): \n",
    "        importances_data = best_model.feature_importances_\n",
    "    elif hasattr(best_model, 'coef_'): \n",
    "        importances_data = np.mean(np.abs(best_model.coef_), axis=0) if best_model.coef_.ndim > 1 else np.abs(best_model.coef_)\n",
    "    \n",
    "    if importances_data is not None and feature_names_processed is not None and \\\n",
    "       len(importances_data) == len(feature_names_processed):\n",
    "        fi_df = pd.DataFrame({\n",
    "            'feature': feature_names_processed,\n",
    "            'importance': importances_data\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    results['Importances'] = fi_df.head(5) if fi_df is not None else \"Not Available\"\n",
    "    \n",
    "    print(f\"--- Pipeline for {model_full_name} complete. ---\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 9: Model Initialization ---\n",
    "print(\"\\n--- Part 9: Initializing Models & Grids ---\")\n",
    "\n",
    "all_models_to_run = []\n",
    "cv_splitter = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "# CLASSIFICATION MODELS\n",
    "all_models_to_run.append({'name': \"Dummy_Frequent\", 'task': 'classification', 'model': DummyClassifier(strategy='most_frequent', random_state=seed), 'grid': None})\n",
    "all_models_to_run.append({'name': \"Dummy_Stratified\", 'task': 'classification', 'model': DummyClassifier(strategy='stratified', random_state=seed), 'grid': None})\n",
    "\n",
    "lr_params = {\n",
    "    'C': [0.1, 1, 10], \n",
    "    'penalty': ['l2'], \n",
    "    'solver': ['lbfgs'], \n",
    "    'class_weight': ['balanced', None], \n",
    "    'max_iter': [1000]\n",
    "}\n",
    "all_models_to_run.append({'name': \"LogisticRegression\", 'task': 'classification', 'model': LogisticRegression(random_state=seed), 'grid': lr_params})\n",
    "\n",
    "dt_params = {\n",
    "    'max_depth': [5, 10, None], \n",
    "    'min_samples_split': [2, 10], \n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "all_models_to_run.append({'name': \"DecisionTree\", 'task': 'classification', 'model': DecisionTreeClassifier(random_state=seed), 'grid': dt_params})\n",
    "\n",
    "rf_gs_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_leaf': [1, 5],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "all_models_to_run.append({'name': \"RandomForest_GS\", 'task': 'classification', 'model': RandomForestClassifier(random_state=seed, n_jobs=-1), 'grid': rf_gs_params})\n",
    "\n",
    "all_models_to_run.append({'name': \"RandomForest_Pruned\", 'task': 'classification', 'model': RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=8, min_samples_split=10, min_samples_leaf=4, \n",
    "    random_state=seed, n_jobs=-1, class_weight='balanced', max_features='sqrt'\n",
    "), 'grid': None})\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "all_models_to_run.append({'name': \"XGBoost\", 'task': 'classification', 'model': XGBClassifier(random_state=seed, objective='multi:softprob', eval_metric='mlogloss'), 'grid': xgb_params})\n",
    "\n",
    "# REGRESSION MODELS\n",
    "all_models_to_run.append({'name': \"Dummy_Mean_Reg\", 'task': 'regression', 'model': DummyRegressor(strategy='mean'), 'grid': None})\n",
    "all_models_to_run.append({'name': \"Dummy_Median_Reg\", 'task': 'regression', 'model': DummyRegressor(strategy='median'), 'grid': None})\n",
    "\n",
    "ridge_params = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "all_models_to_run.append({'name': \"Ridge_Reg\", 'task': 'regression', 'model': Ridge(random_state=seed), 'grid': ridge_params})\n",
    "\n",
    "lasso_params = {'alpha': [0.001, 0.01, 0.1, 1.0]} \n",
    "all_models_to_run.append({'name': \"Lasso_Reg', 'task': 'regression', 'model': Lasso(random_state=seed, max_iter=5000), 'grid': lasso_params}) \n",
    "\n",
    "rf_reg_params = {\n",
    "    'n_estimators': [100, 200], \n",
    "    'max_depth': [5, 10, None], \n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "all_models_to_run.append({'name': \"RandomForest_Reg\", 'task': 'regression', 'model': RandomForestRegressor(random_state=seed, n_jobs=-1), 'grid': rf_reg_params})\n",
    "\n",
    "xgb_reg_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "all_models_to_run.append({'name': \"XGBoost_Reg', 'task': 'regression', 'model': XGBRegressor(random_state=seed, objective='reg:squarederror', eval_metric='rmse'), 'grid': xgb_reg_params})\n",
    "\n",
    "print(\"All models initialized with their respective parameter grids.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 10: Running Modeling Loop ---\n",
    "print(\"\\n--- Running Modeling Loop ---\")\n",
    "\n",
    "final_results_list = []\n",
    "\n",
    "for model_spec in all_models_to_run:\n",
    "    task_y_train = y_train_cv_class_encoded if model_spec['task'] == 'classification' else y_train_cv_reg\n",
    "    task_y_val = y_val_es_class_encoded if model_spec['task'] == 'classification' else y_val_es_reg\n",
    "    task_y_test = y_test_class_encoded if model_spec['task'] == 'classification' else y_test_reg\n",
    "    primary_metric = PRIMARY_METRIC_CLASSIFICATION if model_spec['task'] == 'classification' else PRIMARY_METRIC_REGRESSION\n",
    "    \n",
    "    if task_y_train.empty or (isinstance(X_train_cv_processed, np.ndarray) and X_train_cv_processed.size == 0):\n",
    "        print(f\"Skipping {model_spec['name']} ({model_spec['task']}) due to empty or invalid training data.\")\n",
    "        final_results_list.append({\n",
    "            'Model': f\"{type(model_spec['model']).__name__}_{model_spec['name']}\", \n",
    "            'Task': model_spec['task'], \n",
    "            'CV_Score': np.nan, \n",
    "            'Best_Params': 'Skipped - No Train Data'\n",
    "        })\n",
    "        continue\n",
    "        \n",
    "    result = run_modeling_pipeline(\n",
    "        X_train_cv_processed, task_y_train,\n",
    "        X_val_es_processed, task_y_val,\n",
    "        X_test_processed, task_y_test,\n",
    "        model_spec['model'], \n",
    "        model_name_prefix=f\"_{model_spec['name']}\", \n",
    "        task_type=model_spec['task'],\n",
    "        param_grid=model_spec['grid'], \n",
    "        cv_splitter=cv_splitter if model_spec['grid'] else None, \n",
    "        primary_scoring_metric=primary_metric if model_spec['grid'] else None, \n",
    "        feature_names_processed=processed_feature_names, \n",
    "        random_state_seed=seed\n",
    "    )\n",
    "    final_results_list.append(result)\n",
    "\n",
    "print(\"\\nAll modeling pipelines completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 11: Display Final Results Table ---\n",
    "print(\"\\n--- Part 11: Display Final Results Table ---\")\n",
    "\n",
    "if final_results_list:\n",
    "    valid_results = [res for res in final_results_list if isinstance(res, dict) and 'Model' in res]\n",
    "    \n",
    "    if valid_results:\n",
    "        results_df = pd.DataFrame(valid_results)\n",
    "        \n",
    "        cols_order = ['Model', 'Task', 'CV_Score', 'Accuracy', 'F1-Weighted', 'Precision-W', 'Recall-W', 'ROC_AUC', \n",
    "                      'RMSE', 'MAE', 'R2', 'Best_Params', 'Importances', 'ModelObject']\n",
    "        \n",
    "        results_df_ordered = pd.DataFrame(columns=[col for col in cols_order if col in results_df.columns])\n",
    "        for col in results_df_ordered.columns: \n",
    "            if col in results_df.columns:\n",
    "                results_df_ordered[col] = results_df[col]\n",
    "        \n",
    "        for col in results_df.columns:\n",
    "            if col not in results_df_ordered.columns:\n",
    "                results_df_ordered[col] = results_df[col]\n",
    "        results_df = results_df_ordered \n",
    "\n",
    "        sort_by_cols = ['Task']\n",
    "        sort_ascending = [True]\n",
    "        \n",
    "        if 'F1-Weighted' in results_df.columns and results_df['F1-Weighted'].notna().any():\n",
    "            sort_by_cols.append('F1-Weighted')\n",
    "            sort_ascending.append(False) \n",
    "        if 'RMSE' in results_df.columns and results_df['RMSE'].notna().any(): \n",
    "            sort_by_cols.append('RMSE')\n",
    "            sort_ascending.append(True) \n",
    "        \n",
    "        sort_by_cols_existing = [col for col in sort_by_cols if col in results_df.columns]\n",
    "        sort_ascending_existing = [asc for col, asc in zip(sort_by_cols, sort_ascending) if col in results_df.columns]\n",
    "\n",
    "        if sort_by_cols_existing: \n",
    "             results_df = results_df.sort_values(by=sort_by_cols_existing, ascending=sort_ascending_existing).reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        with pd.option_context('display.max_colwidth', 100): \n",
    "            display(results_df.drop(columns=['ModelObject'], errors='ignore')) \n",
    "\n",
    "        results_csv_path = os.path.join(RESULTS_DIR, 'all_models_full_dataset_summary.csv')\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        print(f\"\\nResults summary saved to: {results_csv_path}\")\n",
    "\n",
    "        best_clf_model = results_df[(results_df['Task'] == 'classification') & (results_df['F1-Weighted'] == results_df['F1-Weighted'].max())]['ModelObject'].iloc[0]\n",
    "        best_reg_model = results_df[(results_df['Task'] == 'regression') & (results_df['RMSE'] == results_df['RMSE'].min())]['ModelObject'].iloc[0]\n",
    "        \n",
    "        joblib.dump(best_clf_model, os.path.join(RESULTS_DIR, 'best_classification_model.pkl'))\n",
    "        joblib.dump(best_reg_model, os.path.join(RESULTS_DIR, 'best_regression_model.pkl'))\n",
    "        print(f\"Best classification model saved to: {os.path.join(RESULTS_DIR, 'best_classification_model.pkl')}\")\n",
    "        print(f\"Best regression model saved to: {os.path.join(RESULTS_DIR, 'best_regression_model.pkl')}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid model results to display or save after filtering.\")\n",
    "else:\n",
    "    print(\"No model results generated.\")\n",
    "\n",
    "print(\"\\n--- Model Training & Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 12: Feature Importance Analysis Helpers ---\n",
    "print(\"\\n--- Part 12: Feature Importance Analysis Helpers ---\")\n",
    "\n",
    "def plot_feature_importances(importance_df, model_name_title, top_n=15, plot_dir=None):\n",
    "    if importance_df is None or importance_df.empty or not isinstance(importance_df, pd.DataFrame):\n",
    "        print(f\"No valid feature importances DataFrame available for {model_name_title}.\")\n",
    "        return\n",
    "    \n",
    "    top_features = importance_df.head(top_n)\n",
    "    plt.figure(figsize=(10, max(6, top_n * 0.45))) \n",
    "    sns.barplot(x='importance', y='feature', data=top_features, palette='viridis')\n",
    "    plt.title(f'Top {top_n} Feature Importances for {model_name_title}', fontsize=15)\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    if plot_dir: \n",
    "        try:\n",
    "            safe_model_name = \"\".join(c if c.isalnum() else \"_\" for c in model_name_title) \n",
    "            plot_path = os.path.join(plot_dir, f'feature_importances_{safe_model_name}.png')\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"Plot saved to: {plot_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot for {model_name_title}: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "def analyze_model_importances(model_name_to_find, task_type_filter, results_list, top_n_display=15, plots_directory=None):\n",
    "    model_result = None\n",
    "    if not results_list: \n",
    "        print(f\"Warning: Results list is empty for {model_name_to_find}.\")\n",
    "        return\n",
    "\n",
    "    for res_dict in results_list:\n",
    "        if isinstance(res_dict, dict) and \\\n",
    "           res_dict.get('Task') == task_type_filter and \\\n",
    "           model_name_to_find == res_dict.get('Model'): \n",
    "            model_result = res_dict\n",
    "            break\n",
    "    \n",
    "    if model_result:\n",
    "        model_full_name = model_result.get('Model', model_name_to_find) \n",
    "        importances_data = model_result.get('Importances')\n",
    "\n",
    "        if isinstance(importances_data, pd.DataFrame) and not importances_data.empty:\n",
    "            print(f\"\\n--- Feature Importances for: {model_full_name} ---\")\n",
    "            display(importances_data.head(top_n_display))\n",
    "            plot_feature_importances(importances_data, model_full_name, top_n=top_n_display, plot_dir=plots_directory)\n",
    "        elif isinstance(importances_data, str) and importances_data == \"Not Available\":\n",
    "            print(f\"\\nFeature importances 'Not Available' for {model_full_name}.\")\n",
    "        else:\n",
    "            print(f\"\\nNo valid feature importances DataFrame for {model_full_name}.\")\n",
    "    else:\n",
    "        print(f\"\\nCould not find results for '{model_name_to_find}'.\")\n",
    "\n",
    "print(\"Feature importance helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 13: Feature Importance Analysis Execution ---\n",
    "print(\"\\n--- Part 13: Executing Feature Importance Analysis ---\")\n",
    "\n",
    "analyze_model_importances(\n",
    "    model_name_to_find=\"XGBClassifier_XGBoost\", \n",
    "    task_type_filter='classification',\n",
    "    results_list=final_results_list, \n",
    "    plots_directory=PLOTS_DIR        \n",
    ")\n",
    "\n",
    "analyze_model_importances(\n",
    "    model_name_to_find=\"XGBRegressor_XGBoost_Reg\",\n",
    "    task_type_filter='regression',\n",
    "    results_list=final_results_list,\n",
    "    plots_directory=PLOTS_DIR\n",
    ")\n",
    "    \n",
    "print(\"\\n--- Feature Importance Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
