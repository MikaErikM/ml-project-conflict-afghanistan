{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Imports & Configuration ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor \n",
    "from xgboost import XGBClassifier, XGBRegressor \n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error, r2_score \n",
    ")\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Part 1: Setup & Configuration ---\")\n",
    "\n",
    "# Configuration Constants\n",
    "RAW_DATA_PATH = 'data/afgh_may25.csv' # file path \n",
    "TAKEOVER_DATE_STR = \"2021-08-15\"\n",
    "GRID_CELL_SIZE_DEG = 1.0\n",
    "HISTORY_LAGS_DAYS = [7, 30, 90]\n",
    "MIN_ACTOR_FREQ = 10\n",
    "MAX_DAYS_SINCE_LAST_EVENT = 180\n",
    "\n",
    "TEST_SET_FRAC = 0.15\n",
    "VALIDATION_SET_FRAC_FROM_DEV = 0.15\n",
    "\n",
    "N_CV_SPLITS = 3\n",
    "PRIMARY_METRIC_CLASSIFICATION = 'f1_weighted'\n",
    "PRIMARY_METRIC_REGRESSION = 'neg_root_mean_squared_error' \n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "RESULTS_DIR = 'results/'\n",
    "PLOTS_DIR = \"plots/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Load and Initial Clean ACLED Data ---\n",
    "print(\"\\n--- Part 2: Load and Initial Clean ACLED Data ---\")\n",
    "try:\n",
    "    df = pd.read_csv(RAW_DATA_PATH, low_memory=False)\n",
    "    print(f\"Loaded ACLED data. Initial shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: ACLED data file '{RAW_DATA_PATH}' not found. Halting.\")\n",
    "    exit()\n",
    "\n",
    "df['event_date'] = pd.to_datetime(df['event_date'], errors='coerce')\n",
    "df.dropna(subset=['event_date'], inplace=True)\n",
    "numeric_cols_convert = ['latitude', 'longitude', 'fatalities', 'geo_precision', 'time_precision']\n",
    "for col in numeric_cols_convert:\n",
    "    if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df['fatalities'] = df['fatalities'].fillna(0).astype(int)\n",
    "df['log_fatalities'] = np.log1p(df['fatalities'])\n",
    "def classify_fatalities(x):\n",
    "    if x == 0: return 'none'\n",
    "    elif x == 1: return 'low'\n",
    "    else: return 'serious'\n",
    "df['fatality_level'] = df['fatalities'].apply(classify_fatalities)\n",
    "df.sort_values('event_date', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Data initially cleaned. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Feature Engineering ---\n",
    "print(\"\\n--- Part 3: Feature Engineering ---\")\n",
    "print(\"Engineering temporal features...\")\n",
    "df['year'] = df['event_date'].dt.year\n",
    "df['month'] = df['event_date'].dt.month\n",
    "df['dayofweek'] = df['event_date'].dt.dayofweek\n",
    "df['dayofyear'] = df['event_date'].dt.dayofyear\n",
    "df['is_post_takeover'] = (df['event_date'] >= pd.to_datetime(TAKEOVER_DATE_STR)).astype(int)\n",
    "\n",
    "print(\"Engineering spatial features (grid_cell_id)...\")\n",
    "def add_grid_cell_id(df_input, lat_col, lon_col, cell_size_deg):\n",
    "    df_out = df_input.copy(); df_with_coords = df_out.dropna(subset=[lat_col, lon_col]).copy()\n",
    "    if df_with_coords.empty: df_out['grid_cell_id'] = f\"cell_NaN_NaN\"; return df_out\n",
    "    min_lat,max_lat,min_lon,max_lon = df_with_coords[lat_col].min(),df_with_coords[lat_col].max(),df_with_coords[lon_col].min(),df_with_coords[lon_col].max()\n",
    "    eps = 1e-9; lat_b = np.arange(min_lat,max_lat+cell_size_deg-eps,cell_size_deg); lon_b = np.arange(min_lon,max_lon+cell_size_deg-eps,cell_size_deg)\n",
    "    if len(lat_b)<2: lat_b=np.array([min_lat,max_lat+eps])\n",
    "    if len(lon_b)<2: lon_b=np.array([min_lon,max_lon+eps])\n",
    "    df_with_coords['lat_idx']=pd.cut(df_with_coords[lat_col],bins=lat_b,labels=False,include_lowest=True,right=False)\n",
    "    df_with_coords['lon_idx']=pd.cut(df_with_coords[lon_col],bins=lon_b,labels=False,include_lowest=True,right=False)\n",
    "    df_with_coords['gc_temp']=df_with_coords.apply(lambda r: f\"c_{int(r['lat_idx'])}_{int(r['lon_idx'])}\" if pd.notna(r['lat_idx']) and pd.notna(r['lon_idx']) else \"c_NaN_NaN\",axis=1)\n",
    "    df_out=df_out.join(df_with_coords[['gc_temp']]); df_out.rename(columns={'gc_temp':'grid_cell_id'},inplace=True); df_out['grid_cell_id'].fillna(\"c_NaN_NaN\",inplace=True)\n",
    "    return df_out\n",
    "df = add_grid_cell_id(df, 'latitude', 'longitude', GRID_CELL_SIZE_DEG)\n",
    "\n",
    "print(\"Engineering actor features (actor1_grouped)...\")\n",
    "def encode_actors_grouped(df_in, actor_col, min_f):\n",
    "    df_o=df_in.copy(); nan_p=\"ACTOR_UNKNOWN\"; df_o[actor_col]=df_o[actor_col].fillna(nan_p); act_c=df_o[actor_col].value_counts(); rare_a=act_c[act_c<min_f].index\n",
    "    df_o[f'{actor_col}_grouped']=df_o[actor_col].apply(lambda x: 'Other_Actor' if x in rare_a else x); return df_o\n",
    "df = encode_actors_grouped(df, 'actor1', MIN_ACTOR_FREQ)\n",
    "\n",
    "print(\"Engineering lagged/trend features (event counts & fatality sums for admin1 & grid_cell_id)...\")\n",
    "def engineer_lagged_features(df_input, unit_col, date_col='event_date', lag_windows=[7, 30, 90]):\n",
    "    df_out = df_input.copy()\n",
    "    id_col_for_count = 'event_id_cnty' if 'event_id_cnty' in df_out.columns else df_out.columns[0]\n",
    "    daily_agg = df_out.groupby([unit_col, pd.Grouper(key=date_col, freq='D')], observed=False).agg(\n",
    "        _daily_event_count=(id_col_for_count, 'size'), \n",
    "        _daily_fatalities_sum=('fatalities', 'sum')\n",
    "    ).reset_index()\n",
    "    daily_agg = daily_agg.set_index([unit_col, date_col]).sort_index()\n",
    "    lagged_results_all_units = []\n",
    "    for unit_name, group_df_unit in daily_agg.groupby(level=0): \n",
    "        group_df_unit = group_df_unit.reset_index(level=0, drop=True) \n",
    "        for lag in lag_windows:\n",
    "            win = f'{lag}D'\n",
    "            group_df_unit[f'event_count_{unit_col}_lag{lag}d'] = group_df_unit['_daily_event_count'].shift(1).rolling(win,min_periods=1).sum().fillna(0).astype(int)\n",
    "            group_df_unit[f'sum_fatalities_{unit_col}_lag{lag}d'] = group_df_unit['_daily_fatalities_sum'].shift(1).rolling(win,min_periods=1).sum().fillna(0)\n",
    "        lagged_results_all_units.append(group_df_unit.reset_index().assign(**{unit_col: unit_name}))\n",
    "    if not lagged_results_all_units: return df_out\n",
    "    processed_daily_lags = pd.concat(lagged_results_all_units, ignore_index=True)\n",
    "    df_out['_day_only_date'] = df_out[date_col].dt.normalize()\n",
    "    cols_to_merge = [unit_col, date_col] + [col for col in processed_daily_lags.columns if 'lag' in col and unit_col in col]\n",
    "    cols_to_merge = list(dict.fromkeys(cols_to_merge))\n",
    "    if date_col in processed_daily_lags.columns: processed_daily_lags[date_col] = pd.to_datetime(processed_daily_lags[date_col]).dt.normalize()\n",
    "    df_out = pd.merge(df_out, processed_daily_lags[cols_to_merge], \n",
    "                      left_on=[unit_col, '_day_only_date'], right_on=[unit_col, date_col],\n",
    "                      how='left', suffixes=('', '_daily'))\n",
    "    if f'{date_col}_daily' in df_out.columns: df_out.drop(columns=[f'{date_col}_daily'], inplace=True)\n",
    "    df_out.drop(columns=['_day_only_date'], inplace=True)\n",
    "    for col in df_out.columns:\n",
    "        if ('lag' in col) and (unit_col in col): df_out[col].fillna(0, inplace=True)\n",
    "    return df_out\n",
    "if 'admin1' in df.columns: df = engineer_lagged_features(df, 'admin1', lag_windows=HISTORY_LAGS_DAYS)\n",
    "if 'grid_cell_id' in df.columns: df = engineer_lagged_features(df, 'grid_cell_id', lag_windows=HISTORY_LAGS_DAYS)\n",
    "\n",
    "print(\"Engineering time since last event features...\")\n",
    "df.sort_values('event_date', inplace=True)\n",
    "if 'admin1' in df.columns:\n",
    "    df.sort_values(['admin1', 'event_date'], inplace=True)\n",
    "    df['days_since_last_event_in_admin1'] = df.groupby('admin1')['event_date'].diff().dt.days.fillna(MAX_DAYS_SINCE_LAST_EVENT).clip(upper=MAX_DAYS_SINCE_LAST_EVENT)\n",
    "if 'grid_cell_id' in df.columns:\n",
    "    df_temp_grid_ts = df[df['grid_cell_id'] != \"c_NaN_NaN\"].copy() \n",
    "    if not df_temp_grid_ts.empty:\n",
    "        df_temp_grid_ts.sort_values(['grid_cell_id', 'event_date'], inplace=True)\n",
    "        df_temp_grid_ts['tsle_grid'] = df_temp_grid_ts.groupby('grid_cell_id')['event_date'].diff().dt.days\n",
    "        df = df.join(df_temp_grid_ts[['tsle_grid']]) # Join on index\n",
    "        df.rename(columns={'tsle_grid': 'days_since_last_event_in_grid_cell'}, inplace=True)\n",
    "    if 'days_since_last_event_in_grid_cell' not in df.columns: \n",
    "        df['days_since_last_event_in_grid_cell'] = MAX_DAYS_SINCE_LAST_EVENT\n",
    "    df['days_since_last_event_in_grid_cell'] = df['days_since_last_event_in_grid_cell'].fillna(MAX_DAYS_SINCE_LAST_EVENT).clip(upper=MAX_DAYS_SINCE_LAST_EVENT)\n",
    "df.sort_values('event_date', inplace=True); df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Feature engineering complete. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Feature Selection & Defining X, y ---\n",
    "print(\"\\n--- Part 4: Feature Selection & Defining X, y ---\")\n",
    "TARGET_COL_CLASSIFICATION = 'fatality_level'\n",
    "TARGET_COL_REGRESSION = 'log_fatalities' # Defined for regression y-vars\n",
    "\n",
    "categorical_features_for_ohe = ['event_type', 'sub_event_type', 'admin1', 'grid_cell_id', 'actor1_grouped',\n",
    "                                'year', 'month', 'dayofweek', 'disorder_type', 'interaction', 'source_scale']\n",
    "numerical_features_base = ['latitude', 'longitude', 'geo_precision', 'time_precision', 'dayofyear', 'is_post_takeover']\n",
    "lag_trend_time_since_features = [col for col in df.columns if ('lag' in col or 'trend' in col or 'days_since_last_event' in col)]\n",
    "categorical_features = [col for col in categorical_features_for_ohe if col in df.columns]\n",
    "numerical_features = list(dict.fromkeys([col for col in numerical_features_base if col in df.columns] + \n",
    "                                         [col for col in lag_trend_time_since_features if col in df.columns]))\n",
    "feature_columns = list(dict.fromkeys([col for col in (categorical_features + numerical_features) if col in df.columns]))\n",
    "critical_originals_for_nan_check = ['latitude', 'longitude', 'geo_precision', 'time_precision', 'event_type', 'sub_event_type', 'admin1']\n",
    "existing_critical_for_nan_check = [col for col in critical_originals_for_nan_check if col in df.columns]\n",
    "if existing_critical_for_nan_check: df.dropna(subset=existing_critical_for_nan_check, inplace=True)\n",
    "print(f\"Selected {len(feature_columns)} features. Shape after NaN drop in critical features: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Data Splitting ---\n",
    "print(\"\\n--- Part 5: Data Splitting ---\")\n",
    "df.sort_values('event_date', inplace=True); df.reset_index(drop=True, inplace=True)\n",
    "n_total = len(df); n_test = int(n_total * TEST_SET_FRAC); n_dev = n_total - n_test\n",
    "n_val = int(n_dev * VALIDATION_SET_FRAC_FROM_DEV); n_train_cv = n_dev - n_val\n",
    "df_train_cv = df.iloc[:n_train_cv]; df_val_es = df.iloc[n_train_cv:n_dev]; df_test = df.iloc[n_dev:]\n",
    "print(f\"Train_CV: {len(df_train_cv)}, Val_ES: {len(df_val_es)}, Test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Label Encoding for Target ---\n",
    "print(\"\\n--- Part 6: Label Encoding Target ---\")\n",
    "le = LabelEncoder()\n",
    "# Ensure all DataFrames for concat are not empty before trying to access TARGET_COL_CLASSIFICATION\n",
    "y_concat_list = []\n",
    "if not df_train_cv.empty: y_concat_list.append(df_train_cv[TARGET_COL_CLASSIFICATION])\n",
    "if not df_val_es.empty: y_concat_list.append(df_val_es[TARGET_COL_CLASSIFICATION])\n",
    "if not df_test.empty: y_concat_list.append(df_test[TARGET_COL_CLASSIFICATION])\n",
    "\n",
    "if y_concat_list:\n",
    "    all_y_labels = pd.concat(y_concat_list, ignore_index=True).dropna().unique()\n",
    "    if len(all_y_labels) > 0: le.fit(all_y_labels)\n",
    "    else: print(\"Warning: No labels to fit encoder after concat & dropna\"); le.fit(['none','low','serious']) \n",
    "else: print(\"Warning: All y-label series for concat are empty\"); le.fit(['none','low','serious'])\n",
    "print(f\"LabelEncoder classes: {le.classes_} -> {np.arange(len(le.classes_))}\")\n",
    "\n",
    "y_train_cv_class_encoded = pd.Series(le.transform(df_train_cv[TARGET_COL_CLASSIFICATION]), index=df_train_cv.index) if not df_train_cv.empty else pd.Series(dtype=int)\n",
    "y_val_es_class_encoded = pd.Series(le.transform(df_val_es[TARGET_COL_CLASSIFICATION]), index=df_val_es.index) if not df_val_es.empty else pd.Series(dtype=int)\n",
    "y_test_class_encoded = pd.Series(le.transform(df_test[TARGET_COL_CLASSIFICATION]), index=df_test.index) if not df_test.empty else pd.Series(dtype=int)\n",
    "y_train_cv_reg = df_train_cv[TARGET_COL_REGRESSION] if not df_train_cv.empty else pd.Series(dtype=float)\n",
    "y_val_es_reg = df_val_es[TARGET_COL_REGRESSION] if not df_val_es.empty else pd.Series(dtype=float)\n",
    "y_test_reg = df_test[TARGET_COL_REGRESSION] if not df_test.empty else pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Preprocessing ---\n",
    "print(\"\\n--- Part 7: Preprocessing ---\")\n",
    "X_train_cv = df_train_cv[feature_columns] if not df_train_cv.empty else pd.DataFrame(columns=feature_columns)\n",
    "X_val_es = df_val_es[feature_columns] if not df_val_es.empty else pd.DataFrame(columns=feature_columns)\n",
    "X_test = df_test[feature_columns] if not df_test.empty else pd.DataFrame(columns=feature_columns)\n",
    "\n",
    "# Refresh cat/num features based on X_train_cv to handle any all-NaN columns dropped or empty X_train_cv\n",
    "if not X_train_cv.empty:\n",
    "    categorical_features = [col for col in categorical_features if col in X_train_cv.columns and X_train_cv[col].nunique(dropna=False) > 0] # dropna=False to count NaN as a level if present before OHE\n",
    "    numerical_features = [col for col in numerical_features if col in X_train_cv.columns and (X_train_cv[col].nunique(dropna=False) > 1 if pd.api.types.is_numeric_dtype(X_train_cv[col]) else True) ] # Ensure some variance for scaler\n",
    "    feature_columns = list(dict.fromkeys(categorical_features + numerical_features))\n",
    "    X_train_cv = X_train_cv[feature_columns]; X_val_es = X_val_es[feature_columns]; X_test = X_test[feature_columns]\n",
    "else: # Handle empty X_train_cv\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    print(\"X_train_cv is empty, preprocessor will not be fitted meaningfully.\")\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=0.005), categorical_features),\n",
    "    ('num', StandardScaler(), numerical_features)], remainder='drop')\n",
    "\n",
    "if not X_train_cv.empty:\n",
    "    print(\"Fitting preprocessor on X_train_cv...\"); preprocessor.fit(X_train_cv)\n",
    "    X_train_cv_processed = preprocessor.transform(X_train_cv)\n",
    "    X_val_es_processed = preprocessor.transform(X_val_es) if not X_val_es.empty else np.array([]).reshape(0, X_train_cv_processed.shape[1])\n",
    "    X_test_processed = preprocessor.transform(X_test) if not X_test.empty else np.array([]).reshape(0, X_train_cv_processed.shape[1])\n",
    "    processed_feature_names = preprocessor.get_feature_names_out()\n",
    "    print(f\"Processed shapes: Train {X_train_cv_processed.shape}, Val {X_val_es_processed.shape}, Test {X_test_processed.shape}\")\n",
    "    print(f\"Number of features after OHE/Scaling: {X_train_cv_processed.shape[1]}\")\n",
    "else: # If X_train_cv was empty\n",
    "    X_train_cv_processed = np.array([]); X_val_es_processed = np.array([]); X_test_processed = np.array([])\n",
    "    processed_feature_names = []\n",
    "    print(\"Skipped fitting/transforming due to empty X_train_cv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Model Training & Evaluation Pipeline Function (Corrected from previous discussions) ---\n",
    "print(\"\\n--- Part 8: Model Training & Evaluation Pipeline ---\")\n",
    "# run_modeling_pipeline function definition from your last working version (the one with clone and careful ES handling)\n",
    "def run_modeling_pipeline(X_train_proc, y_train, X_val_proc, y_val, X_test_proc, y_test, model_instance, model_name_prefix, task_type, param_grid=None, cv_splitter=None, primary_scoring_metric=None, feature_names_processed=None, random_state_seed=42):\n",
    "    model_full_name = f\"{type(model_instance).__name__}{model_name_prefix}\"\n",
    "    print(f\"\\n--- Running Pipeline for: {model_full_name} ({task_type}) ---\")\n",
    "    results = {'Model': model_full_name, 'Task': task_type, 'CV_Score': np.nan, 'Best_Params': '{}'}\n",
    "    best_model = None; current_model_instance = clone(model_instance)\n",
    "    if hasattr(current_model_instance,'random_state') and current_model_instance.random_state is None: current_model_instance.random_state=random_state_seed\n",
    "    if hasattr(current_model_instance,'seed') and current_model_instance.seed is None: current_model_instance.seed=random_state_seed\n",
    "    val_set_avail = isinstance(X_val_proc,np.ndarray) and X_val_proc.size>0 and isinstance(y_val,(pd.Series,np.ndarray)) and len(y_val)>0\n",
    "    orig_model_uses_es = 'early_stopping_rounds' in model_instance.get_params() and model_instance.get_params()['early_stopping_rounds'] is not None\n",
    "    fit_params_gscv={}\n",
    "    if orig_model_uses_es and val_set_avail:\n",
    "        fit_params_gscv['eval_set']=[(X_val_proc,y_val)]; \n",
    "        if type(current_model_instance).__name__.startswith('XGB'): fit_params_gscv['verbose']=False\n",
    "        print(\"    (Val set for GSCV ES)\")\n",
    "    elif orig_model_uses_es and not val_set_avail:\n",
    "        if hasattr(current_model_instance,'early_stopping_rounds'): current_model_instance.early_stopping_rounds=None\n",
    "        print(\"    (No val set, GSCV ES disabled on clone)\")\n",
    "    if param_grid and cv_splitter and primary_scoring_metric:\n",
    "        print(f\"  GSCV for {model_full_name}...\"); gscv=GridSearchCV(current_model_instance,param_grid,cv=cv_splitter,scoring=primary_scoring_metric,n_jobs=-1,verbose=0, error_score='raise') # Added error_score\n",
    "        try: \n",
    "            gscv.fit(X_train_proc,y_train,**fit_params_gscv); best_model=gscv.best_estimator_\n",
    "            results['Best_Params']=str(gscv.best_params_); results['CV_Score']=gscv.best_score_\n",
    "            print(f\"    Best params: {results['Best_Params']}\"); print(f\"    Best CV ({primary_scoring_metric}): {results['CV_Score']:.4f}\")\n",
    "        except Exception as e: print(f\"    ERR GSCV for {model_full_name}: {e}\\n    Fallback default fit.\"); best_model=None \n",
    "    if not best_model: \n",
    "        print(f\"  Training {model_full_name} (no GSCV or GSCV failed).\"); fallback_model=clone(model_instance) \n",
    "        if hasattr(fallback_model,'random_state') and fallback_model.random_state is None: fallback_model.random_state=random_state_seed\n",
    "        if hasattr(fallback_model,'seed') and fallback_model.seed is None: fallback_model.seed=random_state_seed\n",
    "        try:\n",
    "            if orig_model_uses_es and val_set_avail: fallback_model.fit(X_train_proc,y_train,eval_set=[(X_val_proc,y_val)],verbose=False)\n",
    "            elif orig_model_uses_es and not val_set_avail: \n",
    "                if hasattr(fallback_model,'early_stopping_rounds'): fallback_model.early_stopping_rounds=None\n",
    "                # For XGBoost, verbose in fit() can be a boolean or integer.\n",
    "                # Setting to False or 0 usually suppresses output.\n",
    "                fit_verbose = False if type(fallback_model).__name__.startswith('XGB') else True # Default for others\n",
    "                fallback_model.fit(X_train_proc,y_train,verbose=fit_verbose)\n",
    "            else: fallback_model.fit(X_train_proc,y_train)\n",
    "            best_model=fallback_model\n",
    "        except Exception as e: print(f\"    ERR Fit for {model_full_name}: {e}\"); return {**results, 'ModelObject': None, 'Importances': None} \n",
    "    if not best_model: print(f\"MODEL TRAINING FAILED FOR {model_full_name}\"); return {**results, 'ModelObject': None, 'Importances': None}\n",
    "    results['ModelObject']=best_model\n",
    "    # Evaluation\n",
    "    if isinstance(X_test_proc, np.ndarray) and X_test_proc.size > 0 and isinstance(y_test, (pd.Series, np.ndarray)) and len(y_test) > 0:\n",
    "        y_pred=best_model.predict(X_test_proc); \n",
    "        if task_type=='classification':\n",
    "            results.update({'Accuracy':accuracy_score(y_test,y_pred), 'F1-Weighted':f1_score(y_test,y_pred,average='weighted',zero_division=0),\n",
    "                            'Precision-W':precision_score(y_test,y_pred,average='weighted',zero_division=0), 'Recall-W':recall_score(y_test,y_pred,average='weighted',zero_division=0)})\n",
    "            if hasattr(best_model,\"predict_proba\"):\n",
    "                y_prob=best_model.predict_proba(X_test_proc); roc_auc=np.nan\n",
    "                try:\n",
    "                    unique_lbls_test=np.unique(y_test)\n",
    "                    if len(unique_lbls_test)>2 and y_prob.shape[1]==len(unique_lbls_test): roc_auc=roc_auc_score(y_test,y_prob,multi_class='ovr',average='weighted',labels=unique_lbls_test)\n",
    "                    elif y_prob.shape[1]==2: roc_auc=roc_auc_score(y_test,y_prob[:,1])\n",
    "                except ValueError as e_roc: print(f\"  ROC AUC err for {model_full_name}: {e_roc}\")\n",
    "                results['ROC_AUC']=roc_auc\n",
    "            print(f\"    Test F1-W: {results.get('F1-Weighted',np.nan):.4f}, Accuracy: {results.get('Accuracy',np.nan):.4f}, ROC_AUC: {results.get('ROC_AUC',np.nan):.4f}\")\n",
    "        elif task_type=='regression':\n",
    "            results.update({'RMSE':np.sqrt(mean_squared_error(y_test,y_pred)), 'MAE':mean_absolute_error(y_test,y_pred), 'R2':r2_score(y_test,y_pred)})\n",
    "            print(f\"    Test RMSE: {results.get('RMSE',np.nan):.4f}, R2: {results.get('R2',np.nan):.4f}\")\n",
    "    else: print(f\"  Test set empty or invalid for {model_full_name}, skipping test evaluation.\")\n",
    "    # Feature importance\n",
    "    fi_df=None; imp=None\n",
    "    if hasattr(best_model,'feature_importances_'): imp=best_model.feature_importances_\n",
    "    elif hasattr(best_model,'coef_'): imp=np.mean(np.abs(best_model.coef_),axis=0) if best_model.coef_.ndim>1 else np.abs(best_model.coef_)\n",
    "    if imp is not None and feature_names_processed is not None and len(imp)==len(feature_names_processed):\n",
    "        fi_df=pd.DataFrame({'feature':feature_names_processed,'importance':imp}).sort_values('importance',ascending=False).reset_index(drop=True)\n",
    "    results['Importances']=fi_df.head(5) if fi_df is not None else \"Not Available\"\n",
    "    print(f\"--- Pipeline for {model_full_name} complete. ---\")\n",
    "    return results\n",
    "\n",
    "# --- Initialize Models & Grids ---\n",
    "print(\"\\n--- Initializing Models & Grids ---\")\n",
    "all_models_to_run = []\n",
    "cv_s = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "# CLASSIFICATION MODELS\n",
    "all_models_to_run.append({'name': \"Dummy_MF\", 'task': 'classification', 'model': DummyClassifier(strategy='most_frequent', random_state=seed), 'grid': None})\n",
    "all_models_to_run.append({'name': \"Dummy_Strat\", 'task': 'classification', 'model': DummyClassifier(strategy='stratified', random_state=seed), 'grid': None})\n",
    "lr_params = {'C': [0.1, 1, 10], 'penalty': ['l2'], 'solver': ['lbfgs'], 'class_weight': ['balanced', None], 'max_iter': [1000]}\n",
    "all_models_to_run.append({'name': \"LogisticRegression\", 'task': 'classification', 'model': LogisticRegression(random_state=seed), 'grid': lr_params})\n",
    "dt_params = {'max_depth': [5, 10, None], 'min_samples_split': [2, 10], 'class_weight': ['balanced', None]}\n",
    "all_models_to_run.append({'name': \"DecisionTree\", 'task': 'classification', 'model': DecisionTreeClassifier(random_state=seed), 'grid': dt_params})\n",
    "rf_params = {'n_estimators': [100, 200],'max_depth': [5, 10, None],'min_samples_leaf': [1, 5],'class_weight': ['balanced', None]}\n",
    "all_models_to_run.append({'name': \"RandomForest_GS\", 'task': 'classification', 'model': RandomForestClassifier(random_state=seed, n_jobs=-1), 'grid': rf_params}) # GS for GridSearched\n",
    "all_models_to_run.append({'name': \"RandomForest_Pruned\", 'task': 'classification', 'model': RandomForestClassifier(n_estimators=100, max_depth=8, min_samples_split=10, min_samples_leaf=4, random_state=seed, n_jobs=-1, class_weight='balanced', max_features='sqrt'), 'grid': None})\n",
    "xgb_params = {'n_estimators': [100, 200],'max_depth': [3, 5],'learning_rate': [0.05, 0.1],'subsample': [0.8],'colsample_bytree': [0.8]}\n",
    "# IMPORTANT: For XGBoost, early_stopping_rounds is now handled INSIDE run_modeling_pipeline by modifying a CLONE\n",
    "all_models_to_run.append({'name': \"XGBoost\", 'task': 'classification', 'model': XGBClassifier(random_state=seed, objective='multi:softprob', eval_metric='mlogloss', use_label_encoder=False), 'grid': xgb_params})\n",
    "\n",
    "# REGRESSION MODELS\n",
    "all_models_to_run.append({'name': \"Dummy_Mean_Reg\", 'task': 'regression', 'model': DummyRegressor(strategy='mean'), 'grid': None})\n",
    "all_models_to_run.append({'name': \"Dummy_Median_Reg\", 'task': 'regression', 'model': DummyRegressor(strategy='median'), 'grid': None})\n",
    "ridge_params = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "all_models_to_run.append({'name': \"Ridge_Reg\", 'task': 'regression', 'model': Ridge(random_state=seed), 'grid': ridge_params})\n",
    "lasso_params = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
    "all_models_to_run.append({'name': \"Lasso_Reg\", 'task': 'regression', 'model': Lasso(random_state=seed, max_iter=2000), 'grid': lasso_params})\n",
    "rf_reg_params = {'n_estimators': [100, 200], 'max_depth': [5, 10, None], 'min_samples_leaf': [1, 5]}\n",
    "all_models_to_run.append({'name': \"RandomForest_Reg\", 'task': 'regression', 'model': RandomForestRegressor(random_state=seed, n_jobs=-1), 'grid': rf_reg_params})\n",
    "xgb_reg_params = {'n_estimators': [100, 200],'max_depth': [3, 5],'learning_rate': [0.05, 0.1]}\n",
    "all_models_to_run.append({'name': \"XGBoost_Reg\", 'task': 'regression', 'model': XGBRegressor(random_state=seed, objective='reg:squarederror', eval_metric='rmse'), 'grid': xgb_reg_params})\n",
    "\n",
    "# --- Run Modeling Loop ---\n",
    "print(\"\\n--- Running Modeling Loop ---\")\n",
    "final_results_list = []\n",
    "for model_spec in all_models_to_run:\n",
    "    task_y_train = y_train_cv_class_encoded if model_spec['task'] == 'classification' else y_train_cv_reg\n",
    "    task_y_val = y_val_es_class_encoded if model_spec['task'] == 'classification' else y_val_es_reg\n",
    "    task_y_test = y_test_class_encoded if model_spec['task'] == 'classification' else y_test_reg\n",
    "    primary_metric = PRIMARY_METRIC_CLASSIFICATION if model_spec['task'] == 'classification' else PRIMARY_METRIC_REGRESSION\n",
    "    \n",
    "    # Check if training data for this task is valid\n",
    "    if task_y_train.empty or (isinstance(X_train_cv_processed, np.ndarray) and X_train_cv_processed.size == 0):\n",
    "        print(f\"Skipping {model_spec['name']} ({model_spec['task']}) due to empty/invalid training data.\")\n",
    "        final_results_list.append({'Model': f\"{type(model_spec['model']).__name__}_{model_spec['name']}\", 'Task': model_spec['task'], 'CV_Score': np.nan, 'Best_Params': 'Skipped - No Train Data'})\n",
    "        continue\n",
    "        \n",
    "    result = run_modeling_pipeline(\n",
    "        X_train_cv_processed, task_y_train,\n",
    "        X_val_es_processed, task_y_val,\n",
    "        X_test_processed, task_y_test,\n",
    "        model_spec['model'], model_name_prefix=f\"_{model_spec['name']}\", task_type=model_spec['task'],\n",
    "        param_grid=model_spec['grid'], cv_splitter=cv_s if model_spec['grid'] else None,\n",
    "        primary_scoring_metric=primary_metric if model_spec['grid'] else None,\n",
    "        feature_names_processed=processed_feature_names, random_state_seed=seed\n",
    "    )\n",
    "    final_results_list.append(result)\n",
    "\n",
    "# --- Display Final Results Table ---\n",
    "print(\"\\n--- Part 9: Display Final Results Table ---\")\n",
    "if final_results_list:\n",
    "    valid_results = [res for res in final_results_list if isinstance(res, dict) and 'Model' in res]\n",
    "    if valid_results:\n",
    "        results_df = pd.DataFrame(valid_results)\n",
    "        cols_order = ['Model', 'Task', 'CV_Score', 'Accuracy', 'F1-Weighted', 'Precision-W', 'Recall-W', 'ROC_AUC', \n",
    "                      'RMSE', 'MAE', 'R2', 'Best_Params', 'Importances']\n",
    "        # Ensure only existing columns are selected, and in the defined order\n",
    "        results_df_ordered = pd.DataFrame(columns=[col for col in cols_order if col in results_df.columns])\n",
    "        for col in results_df_ordered.columns: # Populate the new ordered df\n",
    "            if col in results_df.columns:\n",
    "                results_df_ordered[col] = results_df[col]\n",
    "        \n",
    "        # Add any columns from results_df that were not in cols_order (e.g. if a new metric was added to results)\n",
    "        for col in results_df.columns:\n",
    "            if col not in results_df_ordered.columns:\n",
    "                results_df_ordered[col] = results_df[col]\n",
    "        results_df = results_df_ordered # Assign back\n",
    "\n",
    "        # Sorting logic corrected in previous response\n",
    "        sort_by_cols = ['Task']\n",
    "        sort_ascending = [True]\n",
    "        if 'F1-Weighted' in results_df.columns and results_df['F1-Weighted'].notna().any():\n",
    "            sort_by_cols.append('F1-Weighted')\n",
    "            sort_ascending.append(False) \n",
    "        if 'RMSE' in results_df.columns and results_df['RMSE'].notna().any(): \n",
    "            sort_by_cols.append('RMSE')\n",
    "            sort_ascending.append(True) \n",
    "        # Filter out sort_by_cols that don't exist in results_df to prevent error\n",
    "        sort_by_cols_existing = [col for col in sort_by_cols if col in results_df.columns]\n",
    "        sort_ascending_existing = [asc for col, asc in zip(sort_by_cols, sort_ascending) if col in results_df.columns]\n",
    "\n",
    "        if sort_by_cols_existing: # Only sort if there's something to sort by\n",
    "             results_df = results_df.sort_values(by=sort_by_cols_existing, ascending=sort_ascending_existing).reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\nModel Performance Summary (Full Dataset):\")\n",
    "        with pd.option_context('display.max_colwidth', 100): \n",
    "            display(results_df)\n",
    "        \n",
    "        results_csv_path = os.path.join(RESULTS_DIR, 'all_models_full_dataset_summary_v3.csv')\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        print(f\"\\nResults summary saved to: {results_csv_path}\")\n",
    "    else:\n",
    "        print(\"No valid model results to display or save.\")\n",
    "else:\n",
    "    print(\"No model results generated.\")\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Feature Importance Analysis (Best Classification & Regression Models)\n",
    "\n",
    "print(\"\\n\\n--- Feature Importance Analysis (Best Full Dataset Models) ---\")\n",
    "\n",
    "# --- Helper Function to Plot Importances ---\n",
    "def plot_feature_importances(importance_df, model_name_title, top_n=15, plot_dir=None):\n",
    "    \n",
    "    if importance_df is None or importance_df.empty or not isinstance(importance_df, pd.DataFrame):\n",
    "        print(f\"No valid feature importances DataFrame available for {model_name_title}.\")\n",
    "        return\n",
    "    \n",
    "    top_features = importance_df.head(top_n)\n",
    "    plt.figure(figsize=(10, max(6, top_n * 0.45))) \n",
    "    sns.barplot(x='importance', y='feature', data=top_features, palette='viridis')\n",
    "    plt.title(f'Top {top_n} Feature Importances for {model_name_title}', fontsize=15)\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    if plot_dir: # plot_dir should be defined and passed from the main script part\n",
    "        try:\n",
    "            safe_model_name = \"\".join(c if c.isalnum() else \"_\" for c in model_name_title) \n",
    "            plot_path = os.path.join(plot_dir, f'feature_importances_{safe_model_name}.png')\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"Feature importance plot saved to: {plot_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot for {model_name_title}: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Helper Function to Find Result and Process Importances ---\n",
    "def analyze_model_importances(model_name_to_find, task_type_filter, results_list, top_n_display=15, plots_directory=None):\n",
    "    model_result = None\n",
    "    # Assuming results_list is defined and passed correctly\n",
    "    if not results_list: # Check if the list itself is None or empty\n",
    "        print(f\"Warning: results_list is empty or None in analyze_model_importances for {model_name_to_find}.\")\n",
    "        return\n",
    "\n",
    "    for res_dict in results_list:\n",
    "        if isinstance(res_dict, dict) and \\\n",
    "           res_dict.get('Task') == task_type_filter and \\\n",
    "           model_name_to_find == res_dict.get('Model'): \n",
    "            model_result = res_dict\n",
    "            break\n",
    "    \n",
    "    if model_result:\n",
    "        model_full_name = model_result.get('Model', model_name_to_find) \n",
    "        importances_data = model_result.get('Importances')\n",
    "\n",
    "        if isinstance(importances_data, pd.DataFrame) and not importances_data.empty:\n",
    "            print(f\"\\n--- Feature Importances for: {model_full_name} ---\")\n",
    "            display(importances_data.head(top_n_display))\n",
    "            plot_feature_importances(importances_data, model_full_name, top_n=top_n_display, plot_dir=plots_directory)\n",
    "        elif isinstance(importances_data, str) and importances_data == \"Not Available\":\n",
    "            print(f\"\\nFeature importances were marked 'Not Available' for {model_full_name}.\")\n",
    "        else:\n",
    "            print(f\"\\nNo valid feature importances DataFrame found for {model_full_name}. Importances data was: {type(importances_data)}\")\n",
    "    else:\n",
    "        print(f\"\\nCould not find results for a '{task_type_filter}' model with exact name '{model_name_to_find}' in the provided results list.\")\n",
    "\n",
    "\n",
    "# --- Analyze Specific Best Models ---\n",
    "\n",
    "# 1. Best Classification Model: XGBClassifier_XGBoost\n",
    "analyze_model_importances(\n",
    "    model_name_to_find=\"XGBClassifier_XGBoost\", \n",
    "    task_type_filter='classification',\n",
    "    results_list=final_results_list, \n",
    "    plots_directory=PLOTS_DIR        \n",
    ")\n",
    "\n",
    "# 2. Best Regression Model: XGBRegressor_XGBoost_Reg\n",
    "analyze_model_importances(\n",
    "    model_name_to_find=\"XGBRegressor_XGBoost_Reg\",\n",
    "    task_type_filter='regression',\n",
    "    results_list=final_results_list,\n",
    "    plots_directory=PLOTS_DIR\n",
    ")\n",
    "    \n",
    "print(\"\\n--- Feature Importance Analysis Cell (Best Models) Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
